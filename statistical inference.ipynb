{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diamond characteristics as predictors of diamond pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamonds dataset by ggplot2 describes different diamond characteristics and diamond prices. It's crucial for people in the diamond business and customers to be able to understand the correct pricing of diamonds. Understanding the best predictors of pricing allow people to ensure they don't get scammed in the business. It also allows us to create models which predict the correct pricing range of the diamonds.\n",
    "\n",
    "We would like to know which dimensional characteristics of diamond have the greatest effect on diamond pricing. To test the relevance of variables to the pricing we are going to employ different variable selection methods such as selection using p-values and LASSO analysis.\n",
    "\n",
    "Let's begin by loading the data and important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9fd8e45e3185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by exploring contents of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diamonds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains information about diamonds\n",
    "* carat: which is unit used to describe weight of diamond\n",
    "* cut: how well diamond has been cut\n",
    "* color: coloring of diamond\n",
    "* clarity: measurement how clear diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "* depth: depth presentage based on dimensions of x, y, z\n",
    "* table: width of the diamond relative to widest point\n",
    "* price: pricing of the diamond in US dollars\n",
    "* x, y, z: dimensions of diamond: length, width and depth respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributional data labels\n",
    "num_labels = ['carat', 'x', 'y', 'z', 'depth', 'price', 'table']\n",
    "#categorial data labels\n",
    "feat_labels = ['cut', 'color', 'clarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand distributional data we first want to analyze its descriptive statistics. Let us first create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"\",\"mean\",\"median\",\"1. quantile\"\n",
    "          , \"3. quantile\", \"std\", \"minimum\", \"maximum\", \"skewness\"]]\n",
    "for i in range(len(num_labels)):\n",
    "    data = df[num_labels[i]].values\n",
    "    table.append([num_labels[i]])\n",
    "    table[i + 1].extend([\n",
    "        np.mean(data),\n",
    "        np.median(data),\n",
    "        np.percentile(data, 25),\n",
    "        np.percentile(data, 75),\n",
    "        np.sqrt(np.var(data)),\n",
    "        np.min(data),\n",
    "        np.max(data),\n",
    "        (np.mean((data - data.mean()) ** 3))/ (np.std(data, ddof = 1) ** 3)\n",
    "    ])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that all variables but depth seem to be heavily skewed to right. This is expected due to all of the variables being non-negative characteristics of the diamond. Also, the variance is kinda high for distributions to resemble a single normal distribution. Something else must be in play.\n",
    "\n",
    "Depth seems to form simple unimodal probability distribution since the quantiles seem to be equally distributed and the skewness is almost zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing the outliers from data\n",
    "def robust_data(data, outlier_persent):\n",
    "    lower = np.partition(data, int(data.size * outlier_persent))\n",
    "    lower = lower[int(data.size * outlier_persent)]\n",
    "    upper = np.partition(data, int(data.size * (1 - outlier_persent)))\n",
    "    upper = upper[int(data.size * (1 - outlier_persent))]\n",
    "    return data[(data > lower) & (data < upper)]\n",
    "\n",
    "def robust_pair(feat, label, outlier_persent):\n",
    "    lower = np.partition(feat, int(feat.size * outlier_persent))\n",
    "    lower = lower[int(feat.size * outlier_persent)]\n",
    "    upper = np.partition(feat, int(feat.size * (1 - outlier_persent)))\n",
    "    upper = upper[int(feat.size * (1 - outlier_persent))]\n",
    "    return feat[(feat > lower) & (feat < upper)] , label[(feat > lower) & (feat < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in num_labels:\n",
    "    data = df[label].values\n",
    "    data = robust_data(data, 0.001)\n",
    "    plt.hist(data, bins = 'rice', range = [np.min(data), np.max(data)] )\n",
    "    plt.title(label + ' distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carat and dimensional x, y and z variables seem to form distributions alike. Each of them has local spikes similarly distributed. This is likely due to diamonds being refined into commonly used sizes. Also for each of them, spikes on the right side get smaller and seem to form a right-leaning tail.\n",
    "\n",
    "Depth actually isn't unimodal. It has many spikes at integer values due to rounding of the values. Still, the integer values seem to form binomial distribution and the other data closely resembles a normal distribution.\n",
    "\n",
    "Table values seem to be a discrete variable. However, there exists values between the integers. This is likely due to table values being rounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in feat_labels:\n",
    "    unique, counts = np.unique(df[label].values, return_counts=True)\n",
    "    ind = np.arange(len(unique))\n",
    "    plt.bar(ind,counts)\n",
    "    plt.xticks(ind, unique)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only significant attention must be given to low quantity of boft only fairly cut diamonds and to the dimmest of diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in num_labels:\n",
    "    if label != 'price':\n",
    "        data = df[label].values\n",
    "        data, price = robust_pair(data, df['price'].values, 0.001)\n",
    "        plt.scatter(data, price, s = 0.1)\n",
    "        plt.xlabel(label)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carat, x, y and z variables seem to have a positive dependency on the price. Because of the distribution of the data, it is hard to see if there exist any trends in depth or table.\n",
    "\n",
    "For curiosity let's also look at categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in feat_labels:\n",
    "    sns.boxplot(x = label, y = 'price', data = df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each categorical variable seems to have really long tails on high price size and they seem to have only a minor effect on the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform a variable selection for predicting diamond pricing. Our hypothesis is that carat of the diamond has a high effect on diamond pricing and the dimensional variables are completely redundant since the information about diamond size is included in carat. We will first examine dimensional variables of the diamond using a linear model.\n",
    "\n",
    "Let us first define the linear model for predicting the price of diamonds.\n",
    "\n",
    "$price_i = \\boldsymbol{\\beta}^T \\boldsymbol{x}_i + \\beta_0 $\n",
    "\n",
    "Where $ \\boldsymbol{\\beta}$ represents the coefficient vector for the model and $\\beta_0$ represents the constant term.\n",
    "\n",
    "We will perform a backward selection and LASSO analysis for the variables. For backward selection, our hypothesis means that carat should be the last variable left for the model as we reduce variables in order of highest p-value. For LASSO analysis carat should be the last variable to reach the zero as we increase the $\\alpha$ penalty of coefficients.\n",
    "\n",
    "Our models assume that the variables have a linear dependence on the price and that they are linearly independent. This might not be the case and we will examine it's implications later. Also, we assume that the variance of all variables is the same. To ensure this we are going to use a standard scaler to standardize the features. This is done by taking the difference of datapoint and mean and dividing it by standard deviation [3].\n",
    "\n",
    "$x_{standard} = (x - \\bar{x})/\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit the linear model to data and analyze the p-values of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df['price'].values\n",
    "labels = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "num_data = df[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p(x, y, y_hat, model_params):\n",
    "    tmp = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    MSE = sum((y - y_hat) ** 2) / (len(tmp) - len(tmp[0]))\n",
    "    std = np.sqrt(MSE * (np.linalg.inv(np.dot(tmp.T,tmp)).diagonal()))\n",
    "    ts = model_params / std\n",
    "    p_vals = [2 * (1 - stats.t.cdf(np.abs(i),(len(tmp)-1))) for i in ts]\n",
    "    return p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for testing purposes\n",
    "def regress(lab, dp):\n",
    "    lm = LinearRegression()\n",
    "    data = df[lab].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data,price,test_size = dp, random_state = 123)\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_train)\n",
    "    lm.fit(x_train, y_train)\n",
    "    predict_data = lm.predict(x_train)\n",
    "    p_vals = calculate_p(x_train, y_train, predict_data, np.append(lm.intercept_,lm.coef_))\n",
    "    return p_vals\n",
    "\n",
    "def print_table(p_vals, lab):\n",
    "    table = [[\"\",\"p-values\"]]\n",
    "    for i in range(len(lab)):\n",
    "        table.append([lab[i], p_vals[i + 1]])\n",
    "    display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "def exe(lab, dp):\n",
    "    print_table(regress(lab, dp),lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with normal backwards selection with small $\\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "exe(labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'depth', 'table', 'x', 'y']\n",
    "exe(labels, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the values are above chosen alpha. This would be the chosen model. Because we wanted to find the best predictors for the price we will continue reducing variables. Because of 64-bit floats aren't accurate enough to capture such a small p-value we will take a percent sample from the dataset. This isn't a reliable method for finding the best variables, but it should indicate what the result should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'table', 'x', 'y', 'z']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'x', 'y', 'z']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'y', 'z']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'z']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat']\n",
    "exe(labels, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LASSO we create a linear predictor. However to make it more robust we penalize the coefficients. Instead of just minimizing the residual we also try to minimize the coefficients. To achieve this we formulate a new cost function.\n",
    "\n",
    "$C = ||y - Xw||^2_2 + \\alpha \\cdot ||w||_1$\n",
    "\n",
    "where $\\alpha$ is the penalization coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat','depth', 'table', 'x', 'y', 'z']\n",
    "X = df[labels].values\n",
    "y = df['price'].values\n",
    "col = []\n",
    "alpha = []\n",
    "X = StandardScaler().fit_transform(X)\n",
    "placeholder = 0.1\n",
    "alphas = np.arange(0.1,3000,0.1)\n",
    "model = Lasso(placeholder, warm_start = True)\n",
    "for i in alphas:\n",
    "    model.set_params(alpha = i)\n",
    "    alpha.append(np.log(i))\n",
    "    model.fit(X,y)\n",
    "    col.append(np.log(np.abs(model.coef_) + 1))\n",
    "plt.plot(alpha, col)\n",
    "plt.legend(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty increases on a logarithmic scale from left to right. The least important variables reach the zero first and the most important last.\n",
    "\n",
    "Unlike the p-values, LASSO seems to value the relevance of depth and table more. This might be due to table and depth being less linearly dependent, but with less covariance than x, y and z. This would also explain why those measurements are taken in the first place.\n",
    "\n",
    "Let us find the best values of alpha using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[labels].values #back to regular data to avoid data leakage\n",
    "model = LassoCV(cv = 10, alphas = alphas, normalize = True).fit(X,y)\n",
    "print(model.alpha_)\n",
    "model = LassoCV(cv = 10, alphas = np.arange(0.001,0.01,0.001), normalize = True).fit(X,y)\n",
    "print(model.alpha_)\n",
    "model = LassoCV(cv = 10, alphas = np.arange(0.0001,0.001,0.0001), normalize = True).fit(X,y)\n",
    "print(model.alpha_)\n",
    "model = LassoCV(cv = 10, alphas = np.arange(0.000001,0.00001,0.000001), normalize = True).fit(X,y)\n",
    "print(model.alpha_)\n",
    "print(\"Carat coef = \" + str(model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it seems that leaving x,y and z variables might be a good method for reducing explanatory variables it seems that they are useful at predicting the prices.\n",
    "\n",
    "Let us analyze our assumptions of data being uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "corr = df[labels].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that x,y,z and carat of the diamond are highly correlated. However, that's not a problem since LASSO is often robust at selecting the best variables then the distinction between the relevance of variables is noticeable. Carat was found to be the most reliable predictor also by the backward selection so we can rely on our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "It seems that our hypothesis of carat being the most important dimensional predictor of the diamond pricing was correct. However, it seems that the prediction of other dimensions being redundant wasn't true. LASSO analysis showed that depth and table of the diamonds seem to be quite important. More importantly, LASSO cross-validation showed that the best model is obtained without the penalization. This means that all variables should be important for predicting the price of the diamond.\n",
    "\n",
    "Using statistical models we have compared dimensional properties of the diamond such as carat, length, width, depth, the width of the diamond relative to the widest point and the depth percentage of the diamond. We found that all variables are valid predictors of the diamond price. It seems though that carat of the diamond is superior at predicting the value of the diamond. Width relative to the widest point and depth percentage were also found to be quite useful.\n",
    "\n",
    "The analysis could be continued by trying non-linear models. Then the variable selection would have had to be done by more efficient methods such as Branch-and-Bound algorithms. Another alternative method could have been least-angle regression. We could also try to divide prices into bins and use gradient boosting algorithm for finding the relevance of different variables.\n",
    "\n",
    "Also, it would be interesting to know how important categorical variables such as cut, color, and clarity are for pricing of diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] R-documentation, diamond prices, https://vincentarelbundock.github.io/Rdatasets/doc/ggplot2/diamonds.html\n",
    "\n",
    "[2] Sklearn-documentation, LASSO, https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html\n",
    "\n",
    "[3] Sklearn-documentation , StandardScaler, https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
